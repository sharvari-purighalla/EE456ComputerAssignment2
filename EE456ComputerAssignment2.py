# -*- coding: utf-8 -*-
"""Computer_Assignment_2_starter_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaICCTcB8vFSIu0R3DkOcwS1UBy06QsD
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ========== Part I: MLE ==========
class MLE:
    def __init__(self, filename, test_ratio=0.1, seed=42):
        """
        Args:
            filename (str): path to data1.csv
            test_ratio (float): test split ratio (e.g., 0.1)
        """
        self.filename = filename
        self.test_ratio = test_ratio
        self.seed = seed
        self.X_train, self.X_test, self.y_train, self.y_test = self._read_and_split()
        self.classes = np.unique(self.y_train)
        self.priors = {}
        self.means = {}
        self.covs = {}

    def _read_and_split(self): # don't edit this part, I write this for you
        df = pd.read_csv(self.filename)
        X = df[["x1", "x2"]].values
        y = df["label"].values

        np.random.seed(self.seed)
        train_idx, test_idx = [], []
        for c in np.unique(y):
            idx = np.where(y == c)[0]
            np.random.shuffle(idx)
            n_test = int(len(idx) * self.test_ratio)
            test_idx.extend(idx[:n_test])
            train_idx.extend(idx[n_test:])
        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

    def visualize_train(self, title="spp-907394064-scatter"):
        """
        Plot training data only.

        Args:
            title (str): plot title following the required naming rule
        """
        # TODO: scatter-plot for each class with different markers/colors, add legend/labels/title
        # Hints:
        # - iterate over self.classes
        # - mask = (self.y_train == c)
        # - plt.scatter(...); plt.xlabel("x1"); plt.ylabel("x2"); plt.legend(); plt.title(title)
        markers = ['o', 's', '^', 'x', 'd'] # enough options
        for i, c in enumerate(self.classes):
            mask = (self.y_train == c)
            plt.scatter(self.X_train[mask, 0],
                        self.X_train[mask, 1],
                        s=25, marker=markers[i % len(markers)],
                        label=f"class {c}")
        plt.xlabel("x1")
        plt.ylabel("x2")
        plt.legend(title="Class")
        plt.title(title)
        plt.tight_layout()
        plt.show()


    def estimate_priors(self):
        """
        Compute priors pi_hat[c] = N_c / N on training set.

        Returns:
            dict: {class -> prior}
        """
        # TODO: fill self.priors for each c in self.classes
        # Return self.priors
        N = len(self.y_train)
        self.priors = {}
        for c in self.classes:
            Nc = np.sum(self.y_train == c)
            self.priors[c] = Nc / N
        return self.priors

    def estimate_means(self):
        """
        Compute class means mu_hat[c] (shape (2,)) on training set.

        Returns:
            dict: {class -> mean vector}
        """
        # TODO: for each class, take mean over X_train[y_train==c]
        self.means = {}
        for c in self.classes:
            Xc = self.X_train[self.y_train == c]
            self.means[c] = Xc.mean(axis=0)
        return self.means

    def estimate_covs(self):
        """
        Compute ML covariances Sigma_hat[c] = (diff^T diff)/N_c.

        Returns:
            dict: {class -> 2x2 covariance matrix}
        """
        # TODO: for each class, compute covariance with ML denominator (N_c, not N_c-1)
        self.covs = {}
        for c in self.classes:
            Xc = self.X_train[self.y_train == c]
            mu = self.means[c]
            diff = Xc - mu
            self.covs[c] = (diff.T @ diff) / Xc.shape[0]
        return self.covs


# ========== Part II: Bayesian Classifier ==========
class BayesianClassifier:
    def __init__(self, priors, means, covs):
        """
        Args:
            priors (dict): {class -> prior}
            means  (dict): {class -> mean (2,)}
            covs   (dict): {class -> cov (2,2)}
        """
        self.priors = priors
        self.means = means
        self.covs = covs
        self.classes = list(priors.keys())

    def log_likelihood(self, x, c):
        """
        Compute log p(x|c) for 2D Gaussian with mean mu_c and covariance Sigma_c.

        Args:
            x (ndarray): shape (2,)
            c (int): class label

        Returns:
            float: log-likelihood value
        """
        # TODO: implement using:
        # - slogdet for log|Sigma|
        # - np.linalg.solve for quadratic form (no explicit inverse)
        
        mu = self.means[c]
        Sigma = self.covs[c]
        # regularization for numerical stability
        Sigma = Sigma + 1e-6 * np.eye(2)

        sign, lodget = np.linalg.slogdet(Sigma)
        if sign <= 0:
            Sigma = Sigma + 1e-3 * np.eye(2)
            sign, lodget = np.linalg.slogdet(Sigma)

        diff = x - mu
        # quadriatic form: (x-mu)^T Sigma^{-1} (x-mu) via solve
        alpha = np.linalg.solve(Sigma, diff)
        quad = float(diff.T @ alpha)

        d = 2
        return -0.5 * (d * np.log(2*np.pi) + lodget + quad)

    def discriminant(self, x, c):
        """
        g_c(x) = log p(x|c) + log pi_c

        Returns:
            float
        """
        # TODO: use self.log_likelihood(x,c) + np.log(self.priors[c])
        return self.log_likelihood(x, c) + np.log(self.priors[c])

    def predict(self, X):
        """
        Predict labels for a batch.

        Args:
            X (ndarray): shape (n_samples, 2)

        Returns:
            ndarray: predicted labels, shape (n_samples,)
        """
        # TODO: for each x, compute g_c(x) for all c and take argmax
        Cs = self.classes
        yhat = []
        for x in X:
            gs = [self.discriminant(x, c) for c in Cs]
            yhat.append(Cs[int(np.argmax(gs))])
        return np.array(yhat, dtype=int)


    def evaluate(self, X_test, y_test):
        """
        Compute Accuracy, NLL, Confusion Matrix, and predictions.

        Returns:
            acc (float)
            nll (float)
            cm  (ndarray): shape (K,K) with rows=true, cols=pred
            y_pred (ndarray): shape (n_test,)
        """
        # TODO:
        # - y_pred = self.predict(X_test)
        # - acc = mean(y_pred == y_test)
        # - nll via log-sum-exp over g_c(x)
        # - confusion matrix via simple counting
        Cs = self.classes
        K = len(Cs)
        c2i = {c:i for i,c in enumerate(Cs)}

        # predictions
        y_pred = self.predict(X_test)
        acc = float(np.mean(y_pred == y_test))

        # NLL via stable log-sum-exp
        n = len(y_test)
        total_log_py = 0.0
        for x, yt in zip(X_test, y_test):
            gs = np.array([self.discriminant(x, c) for c in Cs], dtype=float)
            m = np.max(gs)
            logsumexp = m + np.log(np.sum(np.exp(gs - m)))
            total_log_py += (gs[c2i[yt]] - logsumexp)
        nll = - total_log_py / n

        # confusion matrix
        cm = np.zeros((K, K), dtype=int)
        for yt, yp in zip(y_test, y_pred):
            cm[c2i[yt], c2i[yp]] += 1

        return acc, nll, cm, y_pred

    def plot_decision_regions(self, X_train, X_test, y_test, y_pred, title="xx-123456789-bayes"):
        """
        Draw decision regions + overlay test points and highlight misclassified ones.

        Args:
            X_train (ndarray): (n_train,2) for grid range
            X_test  (ndarray): (n_test,2)
            y_test  (ndarray): (n_test,)
            y_pred  (ndarray): (n_test,)
            title   (str): plot title
        """
        # TODO:
        # - make meshgrid over [min-1, max+1] from training set
        # - predict grid -> reshape -> contourf/pcolormesh
        # - scatter test points per class; circle misclassified
        x_min, x_max = X_train[:,0].min() - 1.0, X_train[:,0].max() + 1.0
        y_min, y_max = X_train[:,1].min() - 1.0, X_train[:,1].max() + 1.0
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),
                            np.linspace(y_min, y_max, 400))
        grid = np.c_[xx.ravel(), yy.ravel()]
        Z = self.predict(grid).reshape(xx.shape)

        plt.figure(figsize=(6,4))
        # decision regions
        plt.contourf(xx, yy, Z, alpha=0.25, levels=len(self.classes))

        # overlay test points per class
        for c, marker in zip(self.classes, ['o','s','^','x','d']):
            mask = (y_test == c)
            plt.scatter(X_test[mask,0], X_test[mask,1],
                        s=30, marker=marker, label=f"test c={c}", edgecolors='k', linewidths=0.3)

        # highlight misclassified
        mis = (y_pred != y_test)
        if np.any(mis):
            plt.scatter(X_test[mis,0], X_test[mis,1],
                        s=80, facecolors='none', edgecolors='red', linewidths=1.5, label="misclassified")

        plt.xlabel("x1")
        plt.ylabel("x2")
        plt.legend(loc="best", fontsize=8)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(f"{title}.png", dpi=200)
        print(f"Decision plot saved as {title}.png")
        plt.show()


# ========== Part III: Perceptron ==========
class Perceptron:
    def __init__(self, filename):
        """
        Args:
            filename (str): path to data2.csv (binary labels in {-1,+1})
        """
        df = pd.read_csv(filename)
        self.X = df[["x1","x2"]].values
        self.y = df["label"].values
        # Augment for bias term
        self.X_aug = np.hstack([self.X, np.ones((self.X.shape[0],1))])
        self.w = np.zeros(self.X_aug.shape[1])
        self.iterations = 0

    def fit(self, eta=0.1, max_iter=10000):
        """
        Train perceptron until all points are correctly classified or max_iter reached.

        Args:
            eta (float): learning rate
            max_iter (int): maximum epochs

        Returns:
            w (ndarray): learned weights (3,)
            iterations (int): epochs used
        """
        # TODO:
        # - initialize w to zeros (already)
        # - loop epochs:
        #     - for each sample, if y_i * (w^T x_i) <= 0: w += eta * y_i * x_i
        #     - count errors; if 0 -> break
        rng = np.random.default_rng(42)
        self.w = np.zeros(self.X_aug.shape[1], dtype=float)
        for epoch in range(1, max_iter + 1):
            errors = 0
            idx = np.arange(self.X_aug.shape[0])
            rng.shuffle(idx)  # better convergence
            for i in idx:
                xi = self.X_aug[i]
                yi = self.y[i]
                if yi * (self.w @ xi) <= 0:
                    self.w += eta * yi * xi
                    errors += 1
            if errors == 0:
                self.iterations = epoch
                return self.w, epoch
        self.iterations = max_iter
        return self.w, max_iter

    def predict(self, X):
        """
        Predict labels for inputs X.

        Args:
            X (ndarray): shape (n_samples,2)

        Returns:
            ndarray: predictions in {-1,+1}, shape (n_samples,)
        """
        # TODO: augment with ones, return sign(X_aug @ self.w)
        X_aug = np.hstack([X, np.ones((X.shape[0],1))])
        s = X_aug @ self.w
        return np.where(s >= 0, 1, -1)

    def plot_decision_boundary(self, title="spp-907394064-perceptron"):
        """
        Plot training points and the learned decision boundary w1*x1 + w2*x2 + b = 0.
        """
        # TODO:
        # - scatter positive/negative class
        # - if w2 != 0: y = -(w1*x + b)/w2; else draw vertical line
        plt.figure(figsize=(6,4))
        pos = (self.y == 1)
        neg = (self.y == -1)
        plt.scatter(self.X[pos,0], self.X[pos,1], s=24, label="+1")
        plt.scatter(self.X[neg,0], self.X[neg,1], s=24, marker="x", label="-1")

        w1, w2, b = self.w
        if abs(w2) > 1e-12:
            xs = np.linspace(self.X[:,0].min()-1, self.X[:,0].max()+1, 300)
            ys = -(w1*xs + b)/w2
            plt.plot(xs, ys, linewidth=2, label="decision boundary")
        else:
            x_line = -b/w1 if abs(w1) > 1e-12 else 0.0
            plt.axvline(x_line, linewidth=2, label="decision boundary")

        plt.xlabel("x1"); plt.ylabel("x2")
        plt.legend()
        plt.title(title)
        plt.tight_layout()
        plt.savefig(f"{title}.png", dpi=200)
        print(f"Saved perceptron plot to {title}.png")
        plt.show()


# ========== Main ==========
if __name__ == "__main__":
    # -------- Part I --------
    mle = MLE("data1.csv")
    mle.visualize_train(title="spp-907394064-scatter")
    priors = mle.estimate_priors()
    means  = mle.estimate_means()
    covs   = mle.estimate_covs()
    print("Priors:", priors)
    print("Means:", means)
    print("Covariances (per class):", {c: "2x2 matrix" for c in mle.classes})

    # -------- Part II --------
    bayes = BayesianClassifier(priors, means, covs)
    # After students implement methods above, they should uncomment the following:
    acc, nll, cm, y_pred = bayes.evaluate(mle.X_test, mle.y_test)
    print("\n=== Bayesian Classifier Results ===")
    print("Accuracy:", acc)
    print("NLL:", nll)
    print("Confusion Matrix (rows=true, cols=pred):\n", cm)
    bayes.plot_decision_regions(mle.X_train, mle.X_test, mle.y_test, y_pred,
                                title="spp-907394064-bayes")

    # -------- Part III --------
    per = Perceptron("data2.csv")
    # Example usage (students may try several etas, but only submit the best plot):
    w, iters = per.fit(eta=0.1, max_iter=10000)
    print(f"Converged in {iters} iterations.")
    per.plot_decision_boundary(title="spp-907394064-perceptron")



