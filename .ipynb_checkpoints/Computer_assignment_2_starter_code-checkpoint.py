# -*- coding: utf-8 -*-
"""Computer_Assignment_2_starter_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaICCTcB8vFSIu0R3DkOcwS1UBy06QsD
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ========== Part I: MLE ==========
class MLE:
    def __init__(self, filename, test_ratio=0.1, seed=42):
        """
        Args:
            filename (str): path to data1.csv
            test_ratio (float): test split ratio (e.g., 0.1)
        """
        self.filename = filename
        self.test_ratio = test_ratio
        self.seed = seed
        self.X_train, self.X_test, self.y_train, self.y_test = self._read_and_split()
        self.classes = np.unique(self.y_train)
        self.priors = {}
        self.means = {}
        self.covs = {}

    def _read_and_split(self): # don't edit this part, I write this for you
        df = pd.read_csv(self.filename)
        X = df[["x1", "x2"]].values
        y = df["label"].values

        np.random.seed(self.seed)
        train_idx, test_idx = [], []
        for c in np.unique(y):
            idx = np.where(y == c)[0]
            np.random.shuffle(idx)
            n_test = int(len(idx) * self.test_ratio)
            test_idx.extend(idx[:n_test])
            train_idx.extend(idx[n_test:])
        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

    def visualize_train(self, title="xx-123456789-scatter"):
        """
        Plot training data only.

        Args:
            title (str): plot title following the required naming rule
        """
        # TODO: scatter-plot for each class with different markers/colors, add legend/labels/title
        # Hints:
        # - iterate over self.classes
        # - mask = (self.y_train == c)
        # - plt.scatter(...); plt.xlabel("x1"); plt.ylabel("x2"); plt.legend(); plt.title(title)
        raise NotImplementedError

    def estimate_priors(self):
        """
        Compute priors pi_hat[c] = N_c / N on training set.

        Returns:
            dict: {class -> prior}
        """
        # TODO: fill self.priors for each c in self.classes
        # Return self.priors
        raise NotImplementedError

    def estimate_means(self):
        """
        Compute class means mu_hat[c] (shape (2,)) on training set.

        Returns:
            dict: {class -> mean vector}
        """
        # TODO: for each class, take mean over X_train[y_train==c]
        raise NotImplementedError

    def estimate_covs(self):
        """
        Compute ML covariances Sigma_hat[c] = (diff^T diff)/N_c.

        Returns:
            dict: {class -> 2x2 covariance matrix}
        """
        # TODO: for each class, compute covariance with ML denominator (N_c, not N_c-1)
        raise NotImplementedError


# ========== Part II: Bayesian Classifier ==========
class BayesianClassifier:
    def __init__(self, priors, means, covs):
        """
        Args:
            priors (dict): {class -> prior}
            means  (dict): {class -> mean (2,)}
            covs   (dict): {class -> cov (2,2)}
        """
        self.priors = priors
        self.means = means
        self.covs = covs
        self.classes = list(priors.keys())

    def log_likelihood(self, x, c):
        """
        Compute log p(x|c) for 2D Gaussian with mean mu_c and covariance Sigma_c.

        Args:
            x (ndarray): shape (2,)
            c (int): class label

        Returns:
            float: log-likelihood value
        """
        # TODO: implement using:
        # - slogdet for log|Sigma|
        # - np.linalg.solve for quadratic form (no explicit inverse)
        raise NotImplementedError

    def discriminant(self, x, c):
        """
        g_c(x) = log p(x|c) + log pi_c

        Returns:
            float
        """
        # TODO: use self.log_likelihood(x,c) + np.log(self.priors[c])
        raise NotImplementedError

    def predict(self, X):
        """
        Predict labels for a batch.

        Args:
            X (ndarray): shape (n_samples, 2)

        Returns:
            ndarray: predicted labels, shape (n_samples,)
        """
        # TODO: for each x, compute g_c(x) for all c and take argmax
        raise NotImplementedError

    def evaluate(self, X_test, y_test):
        """
        Compute Accuracy, NLL, Confusion Matrix, and predictions.

        Returns:
            acc (float)
            nll (float)
            cm  (ndarray): shape (K,K) with rows=true, cols=pred
            y_pred (ndarray): shape (n_test,)
        """
        # TODO:
        # - y_pred = self.predict(X_test)
        # - acc = mean(y_pred == y_test)
        # - nll via log-sum-exp over g_c(x)
        # - confusion matrix via simple counting
        raise NotImplementedError

    def plot_decision_regions(self, X_train, X_test, y_test, y_pred, title="xx-123456789-bayes"):
        """
        Draw decision regions + overlay test points and highlight misclassified ones.

        Args:
            X_train (ndarray): (n_train,2) for grid range
            X_test  (ndarray): (n_test,2)
            y_test  (ndarray): (n_test,)
            y_pred  (ndarray): (n_test,)
            title   (str): plot title
        """
        # TODO:
        # - make meshgrid over [min-1, max+1] from training set
        # - predict grid -> reshape -> contourf/pcolormesh
        # - scatter test points per class; circle misclassified
        raise NotImplementedError


# ========== Part III: Perceptron ==========
class Perceptron:
    def __init__(self, filename):
        """
        Args:
            filename (str): path to data2.csv (binary labels in {-1,+1})
        """
        df = pd.read_csv(filename)
        self.X = df[["x1","x2"]].values
        self.y = df["label"].values
        # Augment for bias term
        self.X_aug = np.hstack([self.X, np.ones((self.X.shape[0],1))])
        self.w = np.zeros(self.X_aug.shape[1])
        self.iterations = 0

    def fit(self, eta=0.1, max_iter=10000):
        """
        Train perceptron until all points are correctly classified or max_iter reached.

        Args:
            eta (float): learning rate
            max_iter (int): maximum epochs

        Returns:
            w (ndarray): learned weights (3,)
            iterations (int): epochs used
        """
        # TODO:
        # - initialize w to zeros (already)
        # - loop epochs:
        #     - for each sample, if y_i * (w^T x_i) <= 0: w += eta * y_i * x_i
        #     - count errors; if 0 -> break
        raise NotImplementedError

    def predict(self, X):
        """
        Predict labels for inputs X.

        Args:
            X (ndarray): shape (n_samples,2)

        Returns:
            ndarray: predictions in {-1,+1}, shape (n_samples,)
        """
        # TODO: augment with ones, return sign(X_aug @ self.w)
        raise NotImplementedError

    def plot_decision_boundary(self, title="xx-123456789-perceptron"):
        """
        Plot training points and the learned decision boundary w1*x1 + w2*x2 + b = 0.
        """
        # TODO:
        # - scatter positive/negative class
        # - if w2 != 0: y = -(w1*x + b)/w2; else draw vertical line
        raise NotImplementedError


# ========== Main ==========
if __name__ == "__main__":
    # -------- Part I --------
    mle = MLE("data1.csv")
    mle.visualize_train(title="xx-123456789-scatter")
    priors = mle.estimate_priors()
    means  = mle.estimate_means()
    covs   = mle.estimate_covs()
    print("Priors:", priors)
    print("Means:", means)
    print("Covariances (per class):", {c: "2x2 matrix" for c in mle.classes})

    # -------- Part II --------
    bayes = BayesianClassifier(priors, means, covs)
    # After students implement methods above, they should uncomment the following:
    # acc, nll, cm, y_pred = bayes.evaluate(mle.X_test, mle.y_test)
    # print("\n=== Bayesian Classifier Results ===")
    # print("Accuracy:", acc)
    # print("NLL:", nll)
    # print("Confusion Matrix (rows=true, cols=pred):\n", cm)
    # bayes.plot_decision_regions(mle.X_train, mle.X_test, mle.y_test, y_pred,
    #                             title="xx-123456789-bayes")

    # -------- Part III --------
    per = Perceptron("data2.csv")
    # Example usage (students may try several etas, but only submit the best plot):
    # w, iters = per.fit(eta=0.1, max_iter=10000)
    # print(f"Converged in {iters} iterations.")
    # per.plot_decision_boundary(title="xx-123456789-perceptron")